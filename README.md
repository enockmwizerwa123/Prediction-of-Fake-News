# Prediction-of-Fake-News using BERT Model
In this work we are going to use BERT model to predict Fake News
`BERT (Bidirectional Encoder Representations from Transformers)` is a pre-trained transformer-based neural network model developed by Google. It is designed to understand the meaning of natural language text by learning contextual relations among words in a sentence.

BERT is trained on a large amount of unlabeled text from the internet, allowing it to understand the nuances and complexity of human language. The model is then fine-tuned on specific natural language processing (NLP) tasks such as question answering, sentiment analysis, and text classification.

One of the main advantages of BERT is that it is bidirectional, which means it can analyze a sentence in both directions, from left to right and from right to left, taking into account the context of each word. This allows the model to capture more complex language structures and better understand the context of the text.

BERT has been shown to achieve state-of-the-art results on a wide range of NLP tasks and has become a popular tool for NLP researchers and practitioners.
